{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T22:42:09.416768Z",
     "start_time": "2018-01-17T22:41:58.980739Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from random import randint\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T22:42:09.465803Z",
     "start_time": "2018-01-17T22:42:09.449790Z"
    }
   },
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "data_dir = os.path.join(parent_dir, 'data')\n",
    "models_dir = os.path.join(parent_dir, 'models')\n",
    "print('working directory: ', os.getcwd())\n",
    "print('data directory:    ', data_dir, )\n",
    "print('models directory:  ', models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T22:44:44.070711Z",
     "start_time": "2018-01-17T22:44:44.065707Z"
    }
   },
   "outputs": [],
   "source": [
    "# change pandas column width so we can see the posts\n",
    "pd.get_option('max_colwidth')\n",
    "pd.set_option('max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:12:08.793131Z",
     "start_time": "2018-01-17T23:12:08.425764Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the data from disk\n",
    "data = pd.read_pickle(os.path.join(data_dir, 'data_clean_4cols_2.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:12:09.676992Z",
     "start_time": "2018-01-17T23:12:09.211661Z"
    }
   },
   "outputs": [],
   "source": [
    "# join all messages by the same candidate\n",
    "candidate_data = pd.DataFrame()\n",
    "candidate_data = (data[['Partei_ABK', 'from_name']].drop_duplicates('from_name')\n",
    "                                                   .set_index('from_name'))\n",
    "candidate_data['messages'] = data.groupby('from_name')['message'].apply(' '.join)\n",
    "candidate_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:33:04.458068Z",
     "start_time": "2018-01-17T23:33:04.439554Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "candidate_data = candidate_data.sample(n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:58:21.544078Z",
     "start_time": "2018-01-17T23:58:21.535572Z"
    }
   },
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:43:44.158664Z",
     "start_time": "2018-01-17T23:43:44.144154Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:42:45.790042Z",
     "start_time": "2018-01-17T23:42:45.767525Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function fo cleaning and tokenization\n",
    "def nlp_clean(messages):\n",
    "    cleaned = []\n",
    "    for message in messages:\n",
    "        message = message.lower()\n",
    "        message = TweetTokenizer().tokenize(message)\n",
    "        words = [word for word in message if (word not in stopwords.words('german')\n",
    "                                              and word not in string.punctuation + '„“‘´'\n",
    "                                              and not word.startswith('http')\n",
    "                                              and not word.isdigit())]\n",
    "        cleaned.append(words)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:42:55.509710Z",
     "start_time": "2018-01-17T23:42:52.175551Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = nlp_clean(candidate_data['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:33:15.947152Z",
     "start_time": "2018-01-17T23:33:15.935644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose a random document/candidate\n",
    "i = randint(0, n-1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:44:10.891431Z",
     "start_time": "2018-01-17T23:44:10.879423Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the document \n",
    "candidate_data['messages'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:44:16.465239Z",
     "start_time": "2018-01-17T23:44:16.436218Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the tokenized and  cleaned document \n",
    "documents[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:54:19.820438Z",
     "start_time": "2018-01-17T23:54:19.780410Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a list containing all words in the corpus\n",
    "vocab = [word for words in documents for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:54:23.163352Z",
     "start_time": "2018-01-17T23:54:23.101308Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep a set of unique words\n",
    "vocab = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T23:54:21.381068Z",
     "start_time": "2018-01-17T23:54:21.373563Z"
    }
   },
   "outputs": [],
   "source": [
    "m = len(vocab)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct term vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:02:55.865785Z",
     "start_time": "2018-01-18T00:02:55.854777Z"
    }
   },
   "outputs": [],
   "source": [
    "target_word = 'steuern'\n",
    "idx = vocab.index(target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:23:45.928661Z",
     "start_time": "2018-01-18T01:23:45.916666Z"
    }
   },
   "outputs": [],
   "source": [
    "t = np.array(np.zeros(m), ndmin=2).T\n",
    "t[vocab.index(target_word)] = 1\n",
    "t#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T15:58:22.205536Z",
     "start_time": "2018-01-17T15:58:22.193535Z"
    }
   },
   "outputs": [],
   "source": [
    "t[idx-5:idx+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:19:26.519773Z",
     "start_time": "2018-01-18T01:19:26.446217Z"
    }
   },
   "outputs": [],
   "source": [
    "window_size=8\n",
    "doc_words = documents[i]\n",
    "middle = randint(window_size, len(doc_words) - window_size - 1)\n",
    "#window_words = [words[c] for c in range(middle - window_size, middle + window_size)]\n",
    "#print(window_words)\n",
    "\n",
    "window_words = []\n",
    "t = []\n",
    "for c in range(middle - window_size, middle + window_size):\n",
    "    window_words.append(doc_words[c])\n",
    "    tt = (np.array(np.zeros(m), ndmin=2).T)\n",
    "    tt[vocab.index(doc_words[c])] = 1\n",
    "    t.append(tt)\n",
    "t[-1][vocab.index(words[c])-5:vocab.index(words[c])+5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:21:30.356137Z",
     "start_time": "2018-01-18T00:21:30.347130Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = np.array(np.zeros(n), ndmin=2).T\n",
    "d[i] = 1\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:21:42.610572Z",
     "start_time": "2018-01-18T00:21:42.600064Z"
    }
   },
   "outputs": [],
   "source": [
    "# p = dimensions of document vectors (no. of features)\n",
    "p = 100\n",
    "D = np.random.rand(p, n)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:48:26.828104Z",
     "start_time": "2018-01-18T00:48:21.006877Z"
    }
   },
   "outputs": [],
   "source": [
    "U = scipy.stats.truncnorm.rvs(-2, 2, loc=0, scale=1, size=(m, p))\n",
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:50:35.228509Z",
     "start_time": "2018-01-18T00:50:35.219022Z"
    }
   },
   "outputs": [],
   "source": [
    "e = np.array(np.dot(D, d), ndmin=2)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:50:46.375981Z",
     "start_time": "2018-01-18T00:50:46.335451Z"
    }
   },
   "outputs": [],
   "source": [
    "k = np.array(np.dot(U, e), ndmin=2)\n",
    "k#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:51:43.567297Z",
     "start_time": "2018-01-18T00:51:43.557812Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(k):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(k) / np.sum(np.exp(k), axis=0)  # axis=1 for row-vector, axis=0 for column-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:51:44.468862Z",
     "start_time": "2018-01-18T00:51:44.447846Z"
    }
   },
   "outputs": [],
   "source": [
    "t_hat = softmax(k)\n",
    "t_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:51:45.617762Z",
     "start_time": "2018-01-18T00:51:45.607771Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(t_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:51:50.071721Z",
     "start_time": "2018-01-18T00:51:50.063714Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(t, t_hat):\n",
    "    return - np.dot(t.T, np.log(t_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:23:49.910600Z",
     "start_time": "2018-01-18T01:23:49.901595Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy_loss(t, t_hat)#[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:55:14.073147Z",
     "start_time": "2018-01-18T00:55:14.062639Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_out = t_hat - t[0]\n",
    "errors_out#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T00:55:29.281439Z",
     "start_time": "2018-01-18T00:55:29.246434Z"
    }
   },
   "outputs": [],
   "source": [
    "errors_middle = np.dot(U.T, errors_out)\n",
    "errors_middle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:45.639248Z",
     "start_time": "2018-01-17T02:11:45.631242Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a learning rate\n",
    "alpha = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:46.737534Z",
     "start_time": "2018-01-17T02:11:46.601566Z"
    }
   },
   "outputs": [],
   "source": [
    "U += - alpha * np.dot(errors_out, e.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:47.647337Z",
     "start_time": "2018-01-17T02:11:47.636329Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D += - alpha * np.dot(errors_middle, d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:08:43.762480Z",
     "start_time": "2018-01-18T01:08:42.767275Z"
    }
   },
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "alpha = 0.025  # learning rate\n",
    "p = 100  # p = dimensions of document vectors (no. of features)\n",
    "m = len(vocab)  # number of words in the corpus \n",
    "\n",
    "D = np.random.rand(p, n)  # matrix of document embeddings\n",
    "U = scipy.stats.truncnorm.rvs(-2, 2, loc=0, scale=1, size=(m, p))  # matrix of softmax weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:14:52.713320Z",
     "start_time": "2018-01-18T01:14:52.116948Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for e in range(epochs):\n",
    "    for i in range(n):\n",
    "        # Feed-forward\n",
    "        d = np.array(np.zeros(n), ndmin=2).T\n",
    "        d[i] = 1\n",
    "        \n",
    "        e = np.array(np.dot(D, d), ndmin=2)\n",
    "        k = np.array(np.dot(U, e), ndmin=2)\n",
    "        t_hat = softmax(k)\n",
    "        \n",
    "        doc_words = documents[i]\n",
    "        middle = randint(window_size, len(doc_words) - window_size - 1)\n",
    "        #window_words = [words[c] for c in range(middle - window_size, middle + window_size)]\n",
    "        #t[] = np.array(np.zeros(len(vocab)), ndmin=2).T\n",
    "        #for w in window_words:\n",
    "        #    t[vocab.index(w)] = 1\n",
    "\n",
    "        #window_words = []\n",
    "        #k = []\n",
    "        \n",
    "        # Backprogation\n",
    "        errors_out = (np.array(np.zeros(m), ndmin=2).T)\n",
    "        errors_middle = (np.array(np.zeros(p), ndmin=2).T)\n",
    "        for c in range(middle - window_size, middle + window_size):\n",
    "            t = (np.array(np.zeros(len(vocab)), ndmin=2).T)\n",
    "            t[vocab.index(doc_words[c])] = 1\n",
    "            errors_out += t_hat - t\n",
    "            errors_middle += np.dot(U.T, errors_out)\n",
    "            \n",
    "            if c == middle:\n",
    "                print(cross_entropy_loss(t, t_hat))\n",
    "\n",
    "        #errors_out = t_hat - t\n",
    "        #errors_middle = np.dot(U.T, errors_out)\n",
    "        U += - alpha * np.dot(errors_out, e.T)\n",
    "        D += - alpha * np.dot(errors_middle, d.T)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the document/candidate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:00:49.462837Z",
     "start_time": "2018-01-18T01:00:48.743339Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "tsne = TSNE(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:01:14.886065Z",
     "start_time": "2018-01-18T01:01:14.858044Z"
    }
   },
   "outputs": [],
   "source": [
    "party_colors = {'AfD': 'xkcd:blue',\n",
    "                'DIE LINKE': 'xkcd:magenta',\n",
    "                'GRÜNE': 'xkcd:grass green',\n",
    "                'CSU': 'xkcd:sky blue',\n",
    "                'CDU': 'xkcd:black',\n",
    "                'FDP': 'xkcd:goldenrod',\n",
    "                'SPD': 'xkcd:red'}\n",
    "candidate_data['color'] = candidate_data['Partei_ABK'].map(party_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:09:30.093691Z",
     "start_time": "2018-01-18T01:09:27.142302Z"
    }
   },
   "outputs": [],
   "source": [
    "D_tsne = tsne.fit_transform(D)\n",
    "plt.figure(num=None, figsize=(10, 8))  # set the figure size\n",
    "plt.scatter(D_tsne[:, 0], D_tsne[:, 1], c=candidate_data['color'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:06:20.307492Z",
     "start_time": "2018-01-18T01:06:17.286484Z"
    }
   },
   "outputs": [],
   "source": [
    "D_tsne = tsne.fit_transform(D)\n",
    "plt.figure(num=None, figsize=(10, 8))  # set the figure size\n",
    "plt.scatter(D_tsne[:, 0], D_tsne[:, 1], c=candidate_data['color'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
