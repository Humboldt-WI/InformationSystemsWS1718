{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Neural Network Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blog post is a guide to help readers build a neural network from the very basics. It starts with an introduction to the concept of a neural networks concept and its early development. A step-by-step coding tutorial follows, through which relevant concepts are illustrated. Later in the post, there is also an introduction on how to build neural networks in Keras. Finally, the reader will find instructions on how to deploy the model via an API to make it accessible to anyone interested on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and its early development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name tells, the idea of neural networks is inspired by how neurons work in the human brain. It is, however, crucial for the readers to know that despite the original motivation of neural networks, the NN models being used today have little resemblance to what a human brain does ???(Warner and Misra, 1996).  In its basic form, neural networks are composed of nodes interconnected to each other in several layers. The basic form of a NN would include an input, a hidden and an output layer. The number of nodes and layers can add to the complexity and efficiency of neural networks.  \n",
    "\n",
    "The McCulloch-Pitts model of neuron in 1943 was one of the earliest simplified version of neural networks. It consisted of a simple neuron which received a weighted sum of inputs and output either zero if the sum was smaller than a threshold or one when it was greater than the threshold. This idea is called firing and is an interesting analogy to what an actual neuron does. Later on, in the early 1960s, Rosenblatt introduced the simple perceptron model. This was a developed version of the McCulloch-Pitts with an input and output layer. However, the linear separablilty limitation ???(Minsky and Papert ,1969) of simple perceptron took away the research interest in neural networks for a while. In the early 1980s, the Hopfield model of content-addressable memory, however, motivated researchers in the area again and later on with the introduction of backpropagation learning algorithm, interest in neural networks research soared. Nowadays, neural nets are used in a variety of applications to tackle problems such as classification, speech and image recognition, control systems and predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mention:\n",
    "    * http://neuralnetworksanddeeplearning.com/index.html                  Michael Nielsen / Dec 2017\n",
    "    * http://www.cristiandima.com/neural-networks-from-scratch-in-python/  CRISTIAN DIMA\n",
    "    * [Rashid, 2016](https://ebook4expert.com/2016/07/12/make-your-own-neural-network-ebook-free-by-tariq-rashid-epubmobi/)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# NN from scratch\n",
    "## Problem Statement\n",
    "The best way to understand how neural networks work is to build one yourself from scratch.\n",
    "The understanding becomes even more comprehensive if there is a particular problem that can be solved using NNs. Therefore let's start our work by taking a look at the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/problem.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are handwritten numbers that you want computer to correctly classify. This would be an easy task for a person but at least for a long period of time was an extremely complicated one for a machine. \n",
    "\n",
    "Even though the computer is faster than the human brain in numeric computations, the brain outperforms the computer in some other tasks. Many of those tasks are related to the human ability for sentience (which is a concept different from intelligence). The trick is to find a way, so that the computer could apply its numeric computation skills to solve these later tasks (at least to some degree).\n",
    "\n",
    "The first step would be to limit the scope of the task. In our particular case the broad task of image recognition will be addressed as a classification problem - a task of giving an object a label from a given set of labels.\n",
    "\n",
    "As we will see during the process of building our own NN, its output is based almost exclusively on application of linear algebra methods. Despite the name (which is sometimes related to the fear of artificial intelligence), neural networks in fact are much more related to statistical methods (like regression analysis or curve fitting) than to the way human brain works [[Stuart Reid, 2014](http://www.turingfinance.com/misconceptions-about-neural-networks/)]. \n",
    "\n",
    "NNs are inspired by human brain only to certain extent. For instance the main element that makes them similar is a multilayer net structure of simple elements that are connected in some way, receiving and transmitting information. But the structure of the human brain is much more complicated, besides it is self-organizing and adaptive in contrast to the fixed manually designed architecture of a NN. Hence, there is a good reason to stop being afraid of neural networks and instead to create one ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neurons_net3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> [Source: [Pixabay.com](https://pixabay.com/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schematic Representation\n",
    "\n",
    "A complex multilayer structure that all neural networks have in common in a simplified way can be depicted using the following picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need in order to implement such a structure is base Python and numpy, a library for numerical computation, which we will be using to do linear algebra. \n",
    "\n",
    "First let's determine the elements of a neural network depicted above: nodes, layers, weights across nodes and activation functions.\n",
    "\n",
    "**Nodes.** A node is basically a point where data points are received, processed and then transferred to the node. A node could be either an endpoint or a redistribution point or even both when iterations are done through the learning algorithm. The number of nodes to use is optional.\n",
    "\n",
    "**Layers.** A layer consists of one or several nodes. The initial layer in the network is called the input layer and it is the entry point through which the data is fed into the neural net. The middle layers are called hidden layer because the computation results of them are not directly visible to someone interacting with the neural net. In the hidden layers, which can range from one to thousands, the features are transformed and most of the structure (both linear and nonlinear) is captured. Finally, there is the final layer, from which results are output. The nodes in each layer are fully interconnected to the ones in the next and the previous layers. \n",
    "\n",
    "In our case we have a structure with 3 layers: input, output and one hidden layer. The number of nodes in the input  (\"i_n\"), hidden (\"h_n\") and output (\"o_n\") layers are 3, 5 and 2 respectively. In Python, such a structure can be represented in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to work with numbers\n",
    "import numpy as np\n",
    "\n",
    "# Determine the structure of the NN\n",
    "i_n = 3\n",
    "h_n = 5\n",
    "o_n = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights.** In order to transfer an input data point to the next layer, a predetermined number (called weight) is stored in each connection from the sender node to the receiver node. Each weight accounts for the impact between the interconnected nodes.\n",
    "\n",
    "Initially, we assign weights between nodes in neighboring layers randomly. This is needed only for the sake of initializing the structure. Later these weights will be changed in order to solve our classification problem. The weight updating will be better described in the following sections.\n",
    "\n",
    "Neural nets will have n-1 matrices of weigths, where n is the number of layers in the NN. You can imagine these weight matrices sitting between two layers representing the strength of the connection between every single node of neighbouring layers. Thus, each of these matrices will be of size f x p, where p is the number of nodes in the preceding layer and f is the number of nodes in the following layer.\n",
    "\n",
    "This becomes more clear once you check the code below that creates 2 matrices of weights:\n",
    "* matrix of weights between input and hidden layers (\"w_i_h\") - 5 by 3 matrix\n",
    "* matrix of weights between hidden and output layers (\"w_h_o\") - 2 by 5 matrix.\n",
    "\n",
    "Such a dimensions of matrices are necessary in order to accomplish matrix and vector multiplications that are done in the following stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63964736, 0.97236245, 0.83944375],\n",
       "       [0.31439566, 0.54254369, 0.0456713 ],\n",
       "       [0.93759599, 0.71292359, 0.11961199],\n",
       "       [0.90587079, 0.0855728 , 0.55046849],\n",
       "       [0.89559465, 0.47349711, 0.42168825]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly define the weights between the layers. \n",
    "w_i_h = np.random.rand(h_n, i_n) # create an array of the given shape and populate it with random values.\n",
    "w_h_o = np.random.rand(o_n, h_n) \n",
    "\n",
    "# Show matrices of randomly assigned weights.\n",
    "w_i_h\n",
    "# w_h_o # uncomment this line in order to see the values for w_h_o.\n",
    "# Use Cmd + / in MacOS and CTRL + / in MS Windows as a shortcut to comment/uncomment lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function.** The remaining element of the NN's structure is an activation function - a function which transforms an input data point that it receives from the previous nodes to an output value which will be the input for the nodes in the next layer. The activation function plays an important role in the efficiency of the neural network as it accounts for non-linearity of data. \n",
    "It is to certain extent inspired by the concept of \"firing\", which means that neurons \"fire\" or transmit information further only if the input surpasses certain threshold. The simplest activation function can be represented by a step function as on the picture below!!!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/step_function.png\" alt=\"Drawing\" style=\"width: 700px\";/> [Source: [Research Gate](https://www.researchgate.net/figure/Three-different-types-of-transfer-function-step-sigmoid-and-linear-in-unipolar-and_306323136)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our NN, we will use a slighlty more elaborate activation function, the sigmoid function (logistic), which allows for more efficient use of the input data. Extended description of various activation functions, their benefits and disadvantages is given in sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine activation function.\n",
    "def sigmoid(x):\n",
    "    # np.exp() calculates the exponential\n",
    "    # of all elements in the input array.\n",
    "    return 1 / (1 + np.exp(-x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw activation function.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# return 100 evenly spaced numbers over an interval from -10 to 10.\n",
    "x = np.linspace(-10, 10, 100) \n",
    "# plot sigmoid function for sampled values.\n",
    "plt.plot(x, sigmoid(x)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "By now we have collected all the elements of the NN. Can we use this structure in order to solve the classification problem stated in the beginning? In order to answer this question we need first to get a better understanding of the data at our disposal. \n",
    "\n",
    "We are trying to check whether NN is able to solve the classification problem using a collection of 70 000 handwritten numbers. Each of this handwritten number is represented as 28x28 image. \n",
    "\n",
    "The original source of the data is \"THE MNIST DATABASE\". A detailed description of the dataset is available at\n",
    "http://yann.lecun.com/exdb/mnist/. There you can also find, for example, a summary of the performance results achieved by various classification algorithms.\n",
    "\n",
    "For the sake of simplicity we suggest obtaining the data from another source:\n",
    "https://pjreddie.com/projects/mnist-in-csv/. Here the original images are saved in CSV, which allows to work with them directly.\n",
    "\n",
    "For the purposes of demonstration below we use a smaller dataset (100 images), which will be expanded at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "raw_data = open(\"data/mnist_train_100.csv\", 'r') # \"r\" stands for \"read only\" mode.\n",
    "data = raw_data.readlines() # read all the lines of a file in a list.\n",
    "raw_data.close() # remove temporal file from the environment in order to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data - check the number of observations.\n",
    "len(data) # length of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,254,254,254,254,255,209,126,67,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,119,230,233,241,216,248,254,254,239,223,132,38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,25,38,0,48,126,245,244,251,254,243,198,131,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,66,113,192,254,254,235,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,29,226,254,37,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,107,254,249,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,186,254,254,130,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,96,216,254,228,100,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,89,212,254,254,210,38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,127,213,255,254,247,66,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,57,254,219,71,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,157,254,202,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,91,251,231,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,98,254,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,30,238,244,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,19,19,4,0,0,0,0,5,228,244,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,142,254,123,14,0,0,0,0,129,254,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,219,254,40,0,0,34,103,221,254,210,26,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,254,233,139,57,232,254,243,166,28,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,94,212,254,254,141,101,41,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a particular observation of the data.\n",
    "data[0] # show observation number 0 from the list (remember that in Python numbering starts from 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A particular observation looks like a string of 785 elements (label of the image + 784 elements for each pixels of a 28x28 image). \n",
    "* Each element representing a pixel is a number from 0 to 255 (from white to black color).\n",
    "* The first element in the line is the label of the image and therefore is a number from 0 to 9.\n",
    "\n",
    "Using `matplotlib`, we can also reconstruct the original image based on the data about each pixel in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to plot the data\n",
    "import matplotlib.pyplot as mpp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ddbf24b1a30d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# break down observation number 0 (comma is used to identify each element).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masfarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# take all the elements starting from the element 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# (exclude element number 0, that corresponds to the label) and reshape them as an array with dimension 28 by 28.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'None'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# show the plot of this array using blue pallete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the data\n",
    "observation = data[0].split(',') # break down observation number 0 (comma is used to identify each element).\n",
    "image = np.asfarray(observation[1:]).reshape((28,28)) # take all the elements starting from the element 1 \n",
    "# (exclude element number 0, that corresponds to the label) and reshape them as an array with dimension 28 by 28.\n",
    "mpp.imshow(image, cmap='Blues', interpolation='None') # show the plot of this array using blue pallete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save an observation of the data as an input to work with.\n",
    "input = np.array(np.asfarray(observation[1:]), ndmin=2).T # save necessary elements in a vertical vector shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the structure of the NN to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look once again at the NN's structure we have created at the beginning of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the data, we can conclude that the structure with 3-5-2 nodes is probably not optimal and therefore should be updated in order to fit the data we have and peculiarities of the classification problem: \n",
    "\n",
    "* For each observation we have 784 elements as an input (label element is excluded). Accordingly, instead of 3 input nodes we should better have 784. \n",
    "* Similarly, as we have 10 different options for the outcome (handwritten numbers are labeled from from 0 to 9) the number of output nodes should be 10 instead of 2. \n",
    "* We also change the number of hidden nodes from 5 to 90. Such a number has been assigned based on some proportionality assumptions which will be checked later: 90 is 9 times higher than 10 and approximately 9 times smaller than 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the new structure of the NN.\n",
    "i_n = 784\n",
    "h_n = 90\n",
    "o_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have new structure of the NN we should reassign the weights - now the size of each weight matrix will increase as we have more nodes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the weights.\n",
    "w_i_h = np.random.rand(h_n, i_n)\n",
    "w_h_o = np.random.rand(o_n, h_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have not used the first element of our observation - the label. It will be necessary to compare the predictions of the NN to the real state of the world and to train the NN to make correct predictions. The target should therefore have the same shape as the output layer of the NN, so that they could be comparable. We can represent the label as a vector of n binary (0 or 1) elements (n corresponds to the number of nodes in the output layer). There should be only one element equal to 1 and the position of this element should correspond to the index number of the label we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target array.\n",
    "target = np.array(np.zeros(o_n), ndmin=2).T\n",
    "target[int(observation[0])] = 1 # int() method returns an integer object from any number or string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect how the target looks like (remember that the label of observations is 5).\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 784), (784, 1), (10, 90), (10, 1))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the sizes of matrices of weights, input and target vectors.\n",
    "w_i_h.shape, input.shape, w_h_o.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforwarding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the structure of the NN updated for the specific task of classifying the numbers depicted on the images, we can run our network in order to get the first predictions that will be represented by a vector of 10 elements. This vector in its turn can be compared to the target.\n",
    "\n",
    "To run the NN, i.e. to feed forward our input data in order to get some predictions, we should follow certain steps:\n",
    "\n",
    "1. Multiply an input vector by a matrix of weights that connects it with the next layer;\n",
    "2. Transform the result using activation function;\n",
    "3. Use the output obtained in the 2nd step as an input vector for the next layer.\n",
    "\n",
    "A sequence of this steps should be repeated n-1 times (where n corresponds to the number of layers). The output of the previous layer will always be the input vector for the next layer. In our case the procedure will happen twice.\n",
    "\n",
    "In the picture bellow !!!, you can see the procedure necessary to obtain the output of the hidden layer. The result of matrix multiplication here is called \"Hidden_Input\". Result of the transformation of \"Hidden_Input\" through activation function is called \"Hidden_Output\".\n",
    "\n",
    "This output will be used as the input vector that should be multiplied by the next weight matrix and transformed through activation function in order to calculate the final output of the NN. If our NN would have more than one hidden layer, the procedure would be repeated more times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/multiplication.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/activation.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the code implementation of all the steps for all layers of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the output of hidden and output layers of our NN.\n",
    "h_input = np.dot(w_i_h, input) # dot() performs matrix multiplication; \"h_input\" stands for \"Hidden_Input\".\n",
    "h_output = sigmoid(h_input) # \"Hidden_Output\" - result after activation function.\n",
    "o_input = np.dot(w_h_o, h_output) # \"Output_Input\" - input used for the next layer.\n",
    "o_output = sigmoid(o_input) # \"Output_Output\" - final output of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show intermediate data and output. \n",
    "# Uncomment the line of interest in order to see the the corresponding object.\n",
    "# h_input\n",
    "# h_output\n",
    "# o_input\n",
    "o_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data treatment good practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we check the output of the NN and the results of each performed step, we can observe that already at the stage of the h_output all the data converts to a vector in which all the values are equal to 1. Such a vector does not provide us with any helpful insight. Apparently, something is wrong with what we have done so far. There could be several reasons for the problem we face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, let's take a look at our sigmoid function once again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JjS7gxYOYDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b2d8438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the output of the sigmoid function will be almost identical once we feed a number bigger than 2. Similarly there is no significant difference between the outputs if numbers used are smaller than -2. Hence the application of sigmoid function to the original data leads to a loss of valuable information. The NN has problems learning something from the inputs, which are almost undifferentiable.  \n",
    "\n",
    "One solution is to transform the input we have. Ideally we should have our data in a range between 0 and 1. It is also desirable to avoid zeros as inputs, because the output of a zero input will always be zero, no matter how large the weigths are, in which case the NN will not be able to use this input to learn.\n",
    "\n",
    "We can perform a transformation of the original data as the one coded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5b47e1c3627e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Good practice transformation of the input values:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masfarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Our values in our input vector are in the range from 0 to 255. Therefore we should divide input vector by 255,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# multiply it by 0,99 and add 0,01 in order to get values in the range from 0,01 to 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'observation' is not defined"
     ]
    }
   ],
   "source": [
    "# Good practice transformation of the input values:\n",
    "input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T \n",
    "# Our values in our input vector are in the range from 0 to 255. Therefore we should divide input vector by 255, \n",
    "# multiply it by 0,99 and add 0,01 in order to get values in the range from 0,01 to 1.\n",
    "\n",
    "# Good practice transformation of the target value:\n",
    "target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "target[int(observation[0])] = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Secondly, we can check our way to randomly assign initial weights:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look once at the function we used to randomly assign weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23828357,  0.79471411,  0.9646791 ,  0.20226973,  0.33866541],\n",
       "       [ 0.70492678,  0.31772727,  0.18652642,  0.60703631,  0.44070431],\n",
       "       [ 0.32261018,  0.24654101,  0.51009432,  0.3691303 ,  0.23014311]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all the weights are positive, while the actual relationship between the features in the data and the values of the output vector can be negative. Hence, the way we employ to assign random weights should allow for negative weights too.\n",
    "\n",
    "Below there are too alternatives how this can be implemented in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31415458,  0.4619982 ,  0.21338529, -1.00177421, -0.72219091],\n",
       "       [-0.01698355,  0.63735232,  0.40266842, -0.20014351, -0.42343207],\n",
       "       [ 0.55214206, -1.03124169,  0.91358413,  0.36368041, -0.24923876]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good practice for initial weights assignment:\n",
    "    \n",
    "alternative1 = np.random.rand(3, 5) - 0.5 \n",
    "# or\n",
    "alternative2 = np.random.normal(0.0, pow(3, -0.5), (3, 5)) \n",
    "# arguments: Mean of the distribution, Standard deviation of the distribution, Output shape.\n",
    "# Second approach is better as it takes in account the standard deviation \n",
    "# that is related to the number of incoming links into a node, 1/(number of incoming links).\n",
    "\n",
    "# alternative1\n",
    "alternative2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the weights in accordance with the best practice:\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the elements assigned in accordance with good practices, we can feedforward the data once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25447848],\n",
       "       [ 0.24667371],\n",
       "       [ 0.24455039],\n",
       "       [ 0.43692272],\n",
       "       [ 0.57460014],\n",
       "       [ 0.82473108],\n",
       "       [ 0.32229881],\n",
       "       [ 0.42787896],\n",
       "       [ 0.21326957],\n",
       "       [ 0.25429989]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run NN to get new classification of the particular observation.\n",
    "h_input = np.dot(w_i_h, input)\n",
    "h_output = sigmoid(h_input)\n",
    "o_input = np.dot(w_h_o, h_output)\n",
    "o_output = sigmoid(o_input)\n",
    "o_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First evaluation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have obtained the output of the NN, we can compare it to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24447848],\n",
       "       [-0.23667371],\n",
       "       [-0.23455039],\n",
       "       [ 0.55307728],\n",
       "       [-0.56460014],\n",
       "       [-0.81473108],\n",
       "       [-0.31229881],\n",
       "       [-0.41787896],\n",
       "       [-0.20326957],\n",
       "       [-0.24429989]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the errors of the classification.\n",
    "o_errors = target - o_output\n",
    "o_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result we would like to achieve should look like as a vector of values where almost all values are negligibly small except for the one value that has the position in the vector corresponding to the index of the true label. \n",
    "\n",
    "It is not the case now. Nevertheless one should remember that so far all the weights have been assigned randomly and no training has been performed yet. In any case, it is not a vector of ones anymore, which an improvement.\n",
    "\n",
    "Thus, we can proceed to the next stage, which is to find out where the errors come from and how they can be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation is a learning algorithm which aims to minimize the errors/cost function of the NN. Through this learning algorithm, the random weights and biases which were initially given to the network will be optimized to give the best output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of each node is the sum of the multiplications of the output of previous nodes by certain weights. Therefore we can associate how much error is coming with every weight and how much error has been brought from each particular node from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this better it is worth imagining the following example:\n",
    "* node 1 in the output layer of the NN should be equal to 0,01 ;\n",
    "* instead the NN is providing us with 0,8.\n",
    "\n",
    "In this case we should do the following:\n",
    "\n",
    "1. Calculate the error of the node (-0,79 in our example);\n",
    "\n",
    "2. Calculate how much error has been brought by every link to this node.\n",
    "\n",
    "For instance if weight w11 is 0,6 and w21 is 0,4 then they are associated with an error of -0,79x(0,6/1) and -0,79x(0,4/1) respectively (see Pictures below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/bp_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculation of how much error is associated with every weight we can obtain the errors for the nodes in the proceeding layer.\n",
    "\n",
    "For instance error term for node 1 in the hidden layer will we equal to:\n",
    "\n",
    "the sum of errors associated with all the weights (w11 and w12 in our case) that link this node with the next layer. (see Picture below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/bp_2.png\"  alt=\"Drawing\" style=\"width: 900px;\"/> [Source: [Rashid, 2016](https://ebook4expert.com/2016/07/12/make-your-own-neural-network-ebook-free-by-tariq-rashid-epubmobi/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we repeat this procedure for all the nodes in all layers we can find out how much every node should be changed.\n",
    "\n",
    "To do so in Python we just need to make multiplication of vector that contain errors by corresponding matrix of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39519721],\n",
       "       [-1.03021742],\n",
       "       [-0.85068716],\n",
       "       [ 1.33854168],\n",
       "       [-0.08195952],\n",
       "       [-0.68480417],\n",
       "       [-0.60794681],\n",
       "       [-0.29001858],\n",
       "       [ 1.5711066 ],\n",
       "       [-0.3642933 ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the errors associated with hidden layer output:\n",
    "h_errors = np.dot(w_h_o.T, o_errors)\n",
    "h_errors[0:10] # errors in the hidden layer - show the first 10 nodes out of 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we improve the weights we have assigned randomly at the beginning, so that the overall result improves?\n",
    "To change the output of any node we should change the weights that connect it with the previous layer.\n",
    "\n",
    "Basically what we want to find out is how much the error in every node changes once we change associated weights. That can be achieved by differentiation of the error function and search for its minimum.\n",
    "\n",
    "The error we want to minimize can be defined as the squared differences between the target value and the output value of the NN. Target value is constant. Output value in its turn is obtained after applying sigmoid function to the sum of weight multiplied by inputs. Following chain rule for derivation our problem can be stated as presented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula2.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula3.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After solving the minimization problem we can update the weights we have assigned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula5.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code this can be represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the matrix for weights between hidden and output layers:\n",
    "w_h_o += np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "# Update the matrix for weights between input and hidden layers:\n",
    "w_i_h += np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the previous code does not display the gif file delete the previous cell and uncomment the following code\n",
    "# import IPython\n",
    "# url = 'https://giphy.com/embed/8tvzvXhB3wcmI'\n",
    "# iframe = '<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>'\n",
    "# IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is something else, we forgot when updating our weights. If we completely change our weights with every new observation - our model learns to predict only the last input. Instead of updating weights 100 % every time we can change them only partially - this way every new observation will bring some new knowledge while the previous one will still be in memory even though updated to certain extent. The bigger the learning rate the more importance has the last observation, the smaller it is the more important is all the previous knowledge. The smaller the steps - the more accurate will be the prediction. At the same time it might take more time to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/learning_rate.png\" alt=\"Drawing\" style=\"width: 600px;\"/> [Source: \"Business Analytics & Data Science Course by Professor S. Lessmann, Chapter 5:\n",
    "Artificial Neural Networks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the learning rate\n",
    "l_r = 0.3\n",
    "\n",
    "# update the weights for the links between the hidden and output layers\n",
    "w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "# update the weights for the links between the input and hidden layers\n",
    "w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it in a bigger scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all the steps done before in a loop, so that we can perform them not just for one observation\n",
    "but for all observations in our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    observation = i.split(',')\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "    target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "    target[int(observation[0])] = 0.99\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    o_errors = target - o_output\n",
    "    h_errors = np.dot(w_h_o.T, o_errors)\n",
    "    \n",
    "    w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "    w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the model with all 100 observations we can test it with new data it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the mnist test data CSV file\n",
    "raw_data_test = open(\"data/mnist_test.csv\", 'r')\n",
    "data_test = raw_data_test.readlines()\n",
    "raw_data_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11206cc18>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112001080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check a particular observation\n",
    "observation = data_test[0].split(',')\n",
    "# print the label\n",
    "print(observation[0])\n",
    "# image the number\n",
    "image = np.asfarray(observation[1:]).reshape((28,28))\n",
    "mpp.imshow(image, cmap='Blues', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10333577],\n",
       "       [ 0.02102201],\n",
       "       [ 0.00853034],\n",
       "       [ 0.08203042],\n",
       "       [ 0.00635177],\n",
       "       [ 0.02326992],\n",
       "       [ 0.02360248],\n",
       "       [ 0.42970462],\n",
       "       [ 0.08602563],\n",
       "       [ 0.04961633]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this observation as an input and run NN with it\n",
    "input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "h_input = np.dot(w_i_h, input)\n",
    "h_output = sigmoid(h_input)\n",
    "o_input = np.dot(w_h_o, h_output)\n",
    "o_output = sigmoid(o_input)\n",
    "\n",
    "o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prediction of NN for this test observation\n",
    "label = np.argmax(o_output)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the neural network using all test dataset\n",
    "\n",
    "# scorecard of the network\n",
    "scorecard = []\n",
    "\n",
    "# go through all the observations in the test data set\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    correct_label = int(observation[0])\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    label = np.argmax(o_output)\n",
    "\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.4806\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is several times better than naive. Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What if we train it more? What does this mean? Introduce local minimum concept.\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.9592\n"
     ]
    }
   ],
   "source": [
    "# The \"big loop\" with epochs\n",
    "for e in range(epochs):\n",
    "    for i in data:\n",
    "        observation = i.split(',')\n",
    "        input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "        target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "        target[int(observation[0])] = 0.99\n",
    "\n",
    "        h_input = np.dot(w_i_h, input)\n",
    "        h_output = sigmoid(h_input)\n",
    "        o_input = np.dot(w_h_o, h_output)\n",
    "        o_output = sigmoid(o_input)\n",
    "\n",
    "        o_errors = target - o_output\n",
    "        h_errors = np.dot(w_h_o.T, o_errors)\n",
    "        w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "        w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))\n",
    "\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "# test\n",
    "scorecard = []\n",
    "\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    correct_label = int(observation[0])\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    label = np.argmax(o_output)\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# calculate accuracy\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() /\n",
    "scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other l_r?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_r = 0.1\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More hidden nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_n = 150\n",
    "\n",
    "# Determine the weights for a bigger matrices\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is much easier to train neural networks where the number of neurons is larger than required. But, with a smaller number of neurons the neural network has much better generalization abilities. It means it will respond correctly for patterns not used for training. If too many neurons are used, then the network can be overtrained on the training patterns, but it will fail on patterns never used in training. With a smaller number of neurons, the network cannot be trained to very small errors, but it may produce much better approximations for new patterns. The most common mistake made by many researchers is that in order to speed up the training process and to reduce the training errors, they use neural networks with a larger number of neurons than required. Such networks would perform very poorly for new patterns not used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = open(\"data/mnist_train.csv\", 'r')\n",
    "data = raw_data.readlines()\n",
    "raw_data.close()\n",
    "\n",
    "# Settings\n",
    "epochs = 2\n",
    "l_r = 0.1\n",
    "h_n = 90\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "This is a problem when the neural network is over trained which would mean that it will fail to recognize patterns which were never used in the training. One of the common reasons for this problem is the use of more than necessary nodes in the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function Types:\n",
    "\n",
    "The activation functions are compared to each other in terms of  their efficient approximation (Schnitger & DasGupta, 1993). There are several types of them but the common ones are the sigmoid, tangent hyperbolic and the RelU functions. The following is a short explanation on the advantages and disavantages of each of these activation functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function: \n",
    "This function squashes the input value into a vaule in the range of [0,1]. It is powerful to compute bolean functions and the smoothness of sigmoid allows for better approxmiation (Schnitger & DasGupta, 1993). However, there are two major drawbacks. First, the sigmoid outputs 0 for high negative values and 1 for high positive values and this results in no signal flows from the nodes because the gradient at these two tails are 0. Second, its output is not centered at 0. This would ultimately result in either all positive gradients on weighte or all negative during the backpropogation.\n",
    "\n",
    "## Tangent hyperbolic: \n",
    "Unlike to sigmoid, this functions output is zero centered and outputs a value in the range of [-1,1]. Although it is more preferred over sigmoid but it sufferes from the same problem because it kills the gradient descent at high or low values.\n",
    "\n",
    "## ReLU: \n",
    "This function is zero whenever input is negative and when the input is positive, its begavior is linear with slope 1. According to Glorot et al. (2011), the ReLU activation functions outperform sigmoidal functions in Deep Neural Networks on text classification and image recognition tasks. ReLU also excells the sigmoid and tangent hyperbolic in accelerating the stochastic gradient descent convergance. However, the ReLU is not prone to the training process and the nodes can die. This means that if a large gradient runs through the nodes, it may update the weights in a way that the node will never be active for any inputs again and this will result in gradient of zero from that node onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent:\n",
    "Gradient descent is one the most poppular algorithms to optimize the neural networks. The name gradient descent is rooted in the procedure where the gradient is repeadtedly evaluated to update the parameters.The objective of the gradient descent is to find weight parameters which will minimize the cost function. In this paper, the focus is mainly on Mini-batch Gradient Descent and the Stochastic Gradient Descent. The Mini-batch Gradient Descent (BGD) is used when there is a large scale training data set where the computation of the cost function over the whole training data set is computationaly expensive. Therefore, a batch of the training data set, say 250 observations, is randomly selected to compute the gradient with the assumption that the observations in the training dataset are correlated. If this procedure is done only for one observation instead of 250, but with 250 iterations, then this procedure is called Stochastic Gradient Descent (SGD) but it is not a common approach since it is not efficient to evaluate the gradient 250 times for one observation than iterating one time over 250 observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
