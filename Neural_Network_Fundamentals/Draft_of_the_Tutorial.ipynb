{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Neural Network Fundamentals. Part 1: NN from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Imagine the following problem.\n",
    "There are handwritten numbers that you want computer to correctly classify. It would be an easy task for a person but an extremely complicated one for a machine, especially, if you want to use some traditional prediction model, like linear regression. Even though the computer is faster than the human brain in numeric computations, the brain far outperforms the computer in some tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/problem.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Some intuition from the Nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People struggled to teach machines to solve this kind of problems for a long time without success.\n",
    "Unless they noticed a very peculiar thing. Nature creatures, even the simple ones, for instance \n",
    "insects, can perform complicated task with very limited brain capacities, which are far below \n",
    "those of the computers. So there is something nature has developed that allows to solve tasks \n",
    "apparently complicated for machines tasks in a smart way.\n",
    "One of the ides that came to mind is to replicate the structure and certain functions of nature beings \n",
    "brain and neurosystem that allow for cognitive process and beyond.\n",
    "A particular example of such structures is neuron system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neurons.png\" alt=\"Drawing\" style=\"width: 600px;\"/> [Source: https://www.jstor.org/stable/pdf/2684922.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neurons_net3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> [Source: https://pixabay.com/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particular detail about how are our cognitive and perceptive processes organised is a complicated \n",
    "structure of simple elements which create a complex net where each element is connected with others \n",
    "receiving and transmitting information. An idea to implement such a structure in order to make \n",
    "predictions gave birth to what we now now as neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early development:\n",
    "##### 1943\n",
    "McCulloch-Pitts model of the neuron. The neuron receives a weighted sum of inputs from connected units, and output a value of one (fires) if this sum is greater than a thresh- old. If the sum is less than the threshold, the model neuron output a zero value.\n",
    "\n",
    "##### Early 1960s\n",
    "Rosenblatt developed a model called simple perceptron. The simple perceptron consists of McCulloch- Pitts model neurons that form two layers, input and output. His model was able to find a solution to classification problems if the problem was linearly separable.\n",
    "Later on Minsky and Papert addressed the linear severability limitation of Rosenblatt model. He knew it himself but could not figure it out.\n",
    "This hindered the process of NNs development.\n",
    "\n",
    "##### 1982\n",
    "Hopfield Model used mainly for optimization problems like travel sales man problem\n",
    "Later on, the idea of backpropagation was introduced and it addressed the earlier problems of the simple perceptron and renewed interest in neural networks. Backpropagation training algorithm is capable of solving nonlinear separable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Applications\n",
    "At the current stage NNs are capable to model many of the capabilities of the human brain and beyond.\n",
    "On a practical level the human brain has many features that are desirable in an electronic computer. The human brain has the ability to generalize from abstract ideas, recognize patterns in the presence of noise, quickly recall memories, and withstand localized damage.\n",
    "\n",
    "Usages of NNS:\n",
    "- identifying underwater sonar contacts\n",
    "- predicting heart problems in patients\n",
    "- diagnosing hypertension\n",
    "- recognizing speech\n",
    "- the preferred tool in predicting protein secondary structures\n",
    "\n",
    "Statisticians use these models to address the same problems:\n",
    "- discriminant analysis\n",
    "- logistic regression\n",
    "- Bayes and other types of classifiers\n",
    "- multiple regression\n",
    "- time series models such as ARIMA and other forecasting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schematic Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above mentioned applications of neural networks have in common a structure that in a simplified way can be depicted using the following picture (see Picture 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the NN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the following sections we will implement such a structure from scratch and use it to solve the classification problem stated in the beginning of the blogpost. All we need in order to do this is Python with a limited number of basic packages. For the sake of making this tutorial even more accessible and also more interactive all the code provided below will be implemented in Jupyter notebook.\n",
    "\n",
    "First let's determine the elements we can see on the Picture 1 and need to collect in order to implement a NN. These elements are:\n",
    "* nodes;\n",
    "* layers;\n",
    "* weights between nodes in neighboring levels;\n",
    "* activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we have a structure with 3 layers: input, output and one hidden layer. The number of nodes in the input  (\"i_n\"), hidden (\"h_n\") and output (\"o_n\") layers are 3, 5 and 2 respectively. Using Python code such a structure can be represented in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to work with numbers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the structure of the NN\n",
    "i_n = 3\n",
    "h_n = 5\n",
    "o_n = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we assign weights between nodes in neighboring layers randomly. This is needed only for the sake of initializing the structure. Later these weights will be changed in order to solve our classification problem. Weights' updating procedure will be described in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be n-1 matrices (where n corresponds to the number of layers in the NN). Each of these matrices will be of a size f by p (where p is the number of nodes in the corresponding preceding layer and f is the number of nodes in the corresponding following layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This becomes more clear once you check the code below that creates 2 matrices of weights:\n",
    "* matrix of weights between input and hidden layers (\"w_i_h\") - 5 by 3 matrix\n",
    "* matrix of weights between hidden and output layers (\"w_h_o\") - 2 by 5 matrix.\n",
    "\n",
    "Such a dimensions of matrices are necessary in order to accomplish matrix and vector multiplications that will be described later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74997179,  0.11739406,  0.28602623],\n",
       "       [ 0.09281727,  0.09172762,  0.74431992],\n",
       "       [ 0.42063356,  0.91765743,  0.91437373],\n",
       "       [ 0.99580619,  0.85346535,  0.70223563],\n",
       "       [ 0.68776875,  0.51399374,  0.01569343]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly define the weights between the layers. \n",
    "# Dimensions of these matrices are determined by the sizes of the layers they are connecting.\n",
    "w_i_h = np.random.rand(h_n, i_n) # create an array of the given shape and populate it with random values.\n",
    "w_h_o = np.random.rand(o_n, h_n) \n",
    "\n",
    "# Show matrices of randomly assigned weights.\n",
    "w_i_h\n",
    "# w_h_o # uncomment this line in order to see the values for w_h_o.\n",
    "# Use Cmd + / in MacOS and CTRL + / in MS Windows as a shortcut to comment/uncomment lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other concept that initially was to certain extent inspired by the way neurosystems in the nature work is the concept of \"firing\". Neurons has a particular characteristic - they are activated, i.e. send signal further or \"fire\" only when they get a signal that is strong enough - stronger than certain threshold. Such a feature of the neurons in the setting of a NN is called activation function. In the easiest case it can be represented by a step function as one on the Picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/step_function.png\" alt=\"Drawing\" style=\"width: 700px\";/> [Source: https://www.researchgate.net/figure/Three-different-types-of-transfer-function-step-sigmoid-and-linear-in-unipolar-and_306323136]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine activation function which is an approximation for \"firing\" of neurons.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) # np.exp() calculates the exponential of all elements in the input array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw this function using `matplotlib.pyplot` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JjS7gxYOYDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f864a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw the function.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100) # return 100 evenly spaced numbers over an interval from -10 to 10.\n",
    "plt.plot(x, sigmoid(x)) # plot sigmoid function for sampled values.\n",
    "plt.show() # show the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now we have collected all the elements of the NN. Can we use this structure in order to solve the classification problem stated in the beginning of the blogpost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer this question we need first to get a better understanding of the data in disposition. \n",
    "\n",
    "We are trying to check whether NN is able to solve the classification problem using a collection of 70 000 handwritten numbers. Each of this handwritten number is represented as 28x28 image. \n",
    "\n",
    "The original source of the data is \"THE MNIST DATABASE\". A detailed description of the dataset as well as the dataset itself can be found under the following link:\n",
    "http://yann.lecun.com/exdb/mnist/. There you can also find a summary of the performance results achieved by various classification algorithms which used this dataset.\n",
    "\n",
    "For the sake of simplicity we suggest obtaining the data from another source:\n",
    "https://pjreddie.com/projects/mnist-in-csv/. Here the original images are transformed in csv.format, which allows to work with them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of demonstration below we use a smaller dataset (100 images), which will be expanded at later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "raw_data = open(\"data/mnist_train_100.csv\", 'r') # \"r\" stands for \"read only\" mode.\n",
    "data = raw_data.readlines() # read all the lines of a file in a list.\n",
    "raw_data.close() # remove temporal file from the environment in order to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data - check the number of observations.\n",
    "len(data) # length of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,254,254,254,254,255,209,126,67,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,119,230,233,241,216,248,254,254,239,223,132,38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,25,38,0,48,126,245,244,251,254,243,198,131,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,66,113,192,254,254,235,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,29,226,254,37,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,107,254,249,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,186,254,254,130,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,96,216,254,228,100,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,89,212,254,254,210,38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,127,213,255,254,247,66,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,57,254,219,71,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,157,254,202,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,91,251,231,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,98,254,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,30,238,244,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,19,19,4,0,0,0,0,5,228,244,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,142,254,123,14,0,0,0,0,129,254,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,219,254,40,0,0,34,103,221,254,210,26,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,254,233,139,57,232,254,243,166,28,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,94,212,254,254,141,101,41,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a particular observation of the data.\n",
    "data[0] # show observation number 0 from the list (remember that in Python numbering starts from 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particular observation looks like a string of 785 elements (label of the image + 784 elements for each pixels of a 28x28 image). \n",
    "\n",
    "Each element representing a pixel is a number from 0 to 255 (from white to black color).\n",
    "\n",
    "The first element in the line is the label of the image and therefore is a number from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `matplotlib.pyplot` package we can also reconstruct the original image based on the data about each pixel in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to plot the data\n",
    "import matplotlib.pyplot as mpp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11214fda0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQFD/D0PAA9CKNoDjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a34f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "observation = data[0].split(',') # break down observation number 0 (comma is used to identify each element).\n",
    "image = np.asfarray(observation[1:]).reshape((28,28)) # take all the elements starting from the element 1 \n",
    "# (exclude element number 0, that corresponds to the label) and reshape them as an array with dimension 28 by 28.\n",
    "mpp.imshow(image, cmap='Blues', interpolation='None') # show the plot of this array using grey pallete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save an observation of the data as an input to work with.\n",
    "input = np.array(np.asfarray(observation[1:]), ndmin=2).T # save necessary elements in a vertical vector shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.04882353],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 1.        ],\n",
       "       [ 0.82141176],\n",
       "       [ 0.49917647],\n",
       "       [ 0.27011765],\n",
       "       [ 0.13423529],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01776471],\n",
       "       [ 0.472     ],\n",
       "       [ 0.90294118],\n",
       "       [ 0.91458824],\n",
       "       [ 0.94564706],\n",
       "       [ 0.84858824],\n",
       "       [ 0.97282353],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.93788235],\n",
       "       [ 0.87576471],\n",
       "       [ 0.52247059],\n",
       "       [ 0.15752941],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.09152941],\n",
       "       [ 0.10705882],\n",
       "       [ 0.15752941],\n",
       "       [ 0.01      ],\n",
       "       [ 0.19635294],\n",
       "       [ 0.49917647],\n",
       "       [ 0.96117647],\n",
       "       [ 0.95729412],\n",
       "       [ 0.98447059],\n",
       "       [ 0.99611765],\n",
       "       [ 0.95341176],\n",
       "       [ 0.77870588],\n",
       "       [ 0.51858824],\n",
       "       [ 0.05270588],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.26623529],\n",
       "       [ 0.44870588],\n",
       "       [ 0.75541176],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.92235294],\n",
       "       [ 0.10705882],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.02552941],\n",
       "       [ 0.12258824],\n",
       "       [ 0.88741176],\n",
       "       [ 0.99611765],\n",
       "       [ 0.15364706],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.02552941],\n",
       "       [ 0.42541176],\n",
       "       [ 0.99611765],\n",
       "       [ 0.97670588],\n",
       "       [ 0.14588235],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.11094118],\n",
       "       [ 0.73211765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.51470588],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.04494118],\n",
       "       [ 0.38270588],\n",
       "       [ 0.84858824],\n",
       "       [ 0.99611765],\n",
       "       [ 0.89517647],\n",
       "       [ 0.39823529],\n",
       "       [ 0.02552941],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.05270588],\n",
       "       [ 0.35552941],\n",
       "       [ 0.83305882],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.82529412],\n",
       "       [ 0.15752941],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.50305882],\n",
       "       [ 0.83694118],\n",
       "       [ 1.        ],\n",
       "       [ 0.99611765],\n",
       "       [ 0.96894118],\n",
       "       [ 0.26623529],\n",
       "       [ 0.02941176],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.23129412],\n",
       "       [ 0.99611765],\n",
       "       [ 0.86023529],\n",
       "       [ 0.28564706],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.03717647],\n",
       "       [ 0.61952941],\n",
       "       [ 0.99611765],\n",
       "       [ 0.79423529],\n",
       "       [ 0.31282353],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.03717647],\n",
       "       [ 0.36329412],\n",
       "       [ 0.98447059],\n",
       "       [ 0.90682353],\n",
       "       [ 0.11482353],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.39047059],\n",
       "       [ 0.99611765],\n",
       "       [ 0.83694118],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.12647059],\n",
       "       [ 0.934     ],\n",
       "       [ 0.95729412],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01388235],\n",
       "       [ 0.08376471],\n",
       "       [ 0.08376471],\n",
       "       [ 0.02552941],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.02941176],\n",
       "       [ 0.89517647],\n",
       "       [ 0.95729412],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.56129412],\n",
       "       [ 0.99611765],\n",
       "       [ 0.48752941],\n",
       "       [ 0.06435294],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.51082353],\n",
       "       [ 0.99611765],\n",
       "       [ 0.83694118],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.06823529],\n",
       "       [ 0.86023529],\n",
       "       [ 0.99611765],\n",
       "       [ 0.16529412],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.142     ],\n",
       "       [ 0.40988235],\n",
       "       [ 0.868     ],\n",
       "       [ 0.99611765],\n",
       "       [ 0.82529412],\n",
       "       [ 0.11094118],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.28952941],\n",
       "       [ 0.99611765],\n",
       "       [ 0.91458824],\n",
       "       [ 0.54964706],\n",
       "       [ 0.23129412],\n",
       "       [ 0.91070588],\n",
       "       [ 0.99611765],\n",
       "       [ 0.95341176],\n",
       "       [ 0.65447059],\n",
       "       [ 0.11870588],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.02164706],\n",
       "       [ 0.37494118],\n",
       "       [ 0.83305882],\n",
       "       [ 0.99611765],\n",
       "       [ 0.99611765],\n",
       "       [ 0.55741176],\n",
       "       [ 0.40211765],\n",
       "       [ 0.16917647],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ],\n",
       "       [ 0.01      ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the input vector.\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fit the draft of the NN's structure to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look once again at the NN's structure we have created at the beginning of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the inspection of the data we can conclude that the structure with 3-5-2 nodes is probably not optimal and therefore should be updated in order to fit the data we have and peculiarities of the classification problem. \n",
    "\n",
    "So, for each observation we have 784 elements as an input (label element is excluded). Accordingly, instead of 3 input nodes we should better have 784. \n",
    "\n",
    "Similarly, as we have 10 different options for the outcome (handwritten numbers are labeled from from 0 to 9) the number of output nodes should be 10 instead of 2. \n",
    "\n",
    "We also change the number of hidden nodes from 5 to 90. Such a number has been assigned based on some proportionality assumptions which will be checked later: 90 is 9 times higher than 10 and approximately 9 times smaller than 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the new structure of the NN.\n",
    "i_n = 784\n",
    "h_n = 90\n",
    "o_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have new structure of the NN we should reassign the weights - now the size of each weight matrix will increase as we have more nodes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the weights.\n",
    "w_i_h = np.random.rand(h_n, i_n)\n",
    "w_h_o = np.random.rand(o_n, h_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have not used the first element of our observation - the label. It will be necessary to compare the predictions of the NN to the real state of the world and to train the NN to make correct predictions. The target should therefore have the same shape as the output layer of the NN, so that they could be comparable. We can represent the label as a vector of n elements (n corresponds to the number of nodes in the output layer), where each element is either 0 or 1. There should be only one element equal to 1 and the position of this element should correspond to the index number of the label we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create target array.\n",
    "target = np.array(np.zeros(o_n), ndmin=2).T\n",
    "target[int(observation[0])] = 1 # int() method returns an integer object from any number or string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect how the target looks like (remember that the label of observations is 5).\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 784), (784, 1), (10, 90), (10, 1))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the sizes of matrices of weights, input and target vectors.\n",
    "w_i_h.shape, input.shape, w_h_o.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the structure of the NN updated for the specific task of classifying the numbers depicted on the images, we can run our network in order to get the first predictions that will be represented by a vector of 10 elements. This vector in its turn can be compared to the target.\n",
    "\n",
    "To run the NN, i.e. to feed forward our input data in order to get some predictions, we should follow certain steps:\n",
    "\n",
    "1. Multiply an input vector by a matrix of weights that connects it with the next layer;\n",
    "2. Transform the result using activation function;\n",
    "3. Use the output obtained in the 2nd step as an input vector for the next layer.\n",
    "\n",
    "A sequence of this steps should be repeated n-1 times (where n corresponds to the number of layers). The output of the previous layer will always be the input vector for the next layer. In our case the procedure will happen twice.\n",
    "\n",
    "On the Picture bellow you can see the procedure necessary to obtain the output of the hidden layer.\n",
    "The result of matrix multiplication here is called \"Hidden_Input\". Transformation of \"Hidden_Input\" through activation function is called \"Hidden_Output\".\n",
    "\n",
    "This output will be used as the input vector that should be multiplied by the next weigh matrix and transformed through activation function in order to calculate the final output of the NN. If our NN would have more than 1 hidden layer, the procedure would be repeated more times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"pics/multiplication.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"pics/activation.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the code implementation of all the steps for all layers of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the output of hidden and output layers of our NN.\n",
    "h_input = np.dot(w_i_h, input) # dot() performs matrix multiplication; \"h_input\" stands for \"Hidden_Input\".\n",
    "h_output = sigmoid(h_input) # \"Hidden_Output\" - result after activation function.\n",
    "o_input = np.dot(w_h_o, h_output) # \"Output_Input\" - input used for the next layer.\n",
    "o_output = sigmoid(o_input) # \"Output_Output\" - final output of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show intermediate data and output. \n",
    "# Uncomment the line of interest in order to see the the corresponding object.\n",
    "# h_input\n",
    "# h_output\n",
    "# o_input\n",
    "o_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why don't we get what we expected? Data treatment best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we check the output of the NN and the results of each performed step, we can observe that already at the stage of the h_output all the data converts to a vector of ones. A vector of equal values does not provide us with any inside that is helpful for the considered classification problem. Apparently, something is wrong with what we have done so far. There could be several reasons for the problem we face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First of all, let's take a look at our sigmoid function once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JjS7gxYOYDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f376a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the output of the sigmoid function will be almost identical once we feed a number bigger than 2. Similarly there is no significant difference between the outputs if numbers used are smaller than -2. Hence application of sigmoid function to the original data leads to a lose of valuable information - NN has problems to learn something from the inputs that are almost undifferentiable.  \n",
    "\n",
    "One solution is to transform the input we have. Ideally we should have our data in a range between 0 and 1. It is desirable to avoid zeros, because the result of multiplication of an input equal to 0 by whichever weight will always be 0, hence NN will not be able to use this input to learn.\n",
    "\n",
    "We can perform a transformation of the original data as the one coded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good practice transformation of the input values:\n",
    "input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T \n",
    "# Our values in our input vector are in the range from 0 to 255. Therefore we should divide input vector by 255, \n",
    "# multiply it by 0,99 and add 0,01 in order to get values in the range from 0,01 to 1.\n",
    "\n",
    "# Good practice transformation of the target value:\n",
    "target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "target[int(observation[0])] = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly, we can check our way to randomly assign initial weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look once at the function we used to randomly assign weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0929566 ,  0.80399214,  0.19298353,  0.09383669,  0.29774564],\n",
       "       [ 0.2391368 ,  0.5695176 ,  0.47187771,  0.00741874,  0.05038327],\n",
       "       [ 0.48572416,  0.13382937,  0.70382773,  0.41792049,  0.47215276]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all the weights are positive, while actual relationship between the features in the data and the values of the output vector can be negative. Hence, the way we employ to assign random weights should allow for negative weights too. \n",
    "\n",
    "Below there are too alternatives how this can be implemented in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69263818,  1.01567791,  0.36971371, -0.86126302,  0.22298794],\n",
       "       [-1.13684782, -0.12144631,  0.55360582, -0.50567714, -0.57922153],\n",
       "       [-0.78691826,  0.14246747, -0.17866024, -0.32111769, -0.03961589]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good practice for initial weights assignment:\n",
    "    \n",
    "alternative1 = np.random.rand(3, 5) - 0.5 \n",
    "# or\n",
    "alternative2 = np.random.normal(0.0, pow(3, -0.5), (3, 5)) \n",
    "# arguments: Mean of the distribution, Standard deviation of the distribution, Output shape.\n",
    "# Second approach is better as it takes in account the standard deviation \n",
    "# that is related to the number of incoming links into a node, 1/(number of incoming links).\n",
    "\n",
    "# alternative1\n",
    "alternative2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the weights in accordance with the best practice:\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have all the elements assigned in accordance with the best practices, we can feedforward the data once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67064173],\n",
       "       [ 0.97389708],\n",
       "       [ 0.52578025],\n",
       "       [ 0.74854766],\n",
       "       [ 0.42091442],\n",
       "       [ 0.90316051],\n",
       "       [ 0.68882403],\n",
       "       [ 0.70197546],\n",
       "       [ 0.03486962],\n",
       "       [ 0.88821824]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run NN to get new classification of the particular observation.\n",
    "h_input = np.dot(w_i_h, input)\n",
    "h_output = sigmoid(h_input)\n",
    "o_input = np.dot(w_h_o, h_output)\n",
    "o_output = sigmoid(o_input)\n",
    "o_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good our results are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have obtained the output of the NN, we can compare it to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.66064173],\n",
       "       [-0.96389708],\n",
       "       [-0.51578025],\n",
       "       [ 0.24145234],\n",
       "       [-0.41091442],\n",
       "       [-0.89316051],\n",
       "       [-0.67882403],\n",
       "       [-0.69197546],\n",
       "       [-0.02486962],\n",
       "       [-0.87821824]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the errors of the classification.\n",
    "o_errors = target - o_output\n",
    "o_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result we would like to achieve should look like as a vector of values where almost all values are negligibly small except for the one value that has the position in the vector corresponding to the index of the label of the image. \n",
    "\n",
    "It is not the case now. Nevertheless one should remember that so far all the weights have been assigned randomly and no training has been performed yet. However, is not a vector of ones anymore. \n",
    "\n",
    "Hence, we can proceed to the next stage, which is to find out where do the errors come from and how they can be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of each node is the sum of the multiplications of the output of previous nodes by certain weights. Therefore we can associate how much error is coming with every weight and how much error has been brought from each particular node from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this better it is worth imagining the following example:\n",
    "* node 1 in the output layer of the NN should be equal to 0,01 ;\n",
    "* instead the NN is providing us with 0,8.\n",
    "\n",
    "In this case we should do the following:\n",
    "\n",
    "1. Calculate the error of the node (-0,79 in our example);\n",
    "\n",
    "2. Calculate how much error has been brought by every link to this node.\n",
    "\n",
    "For instance if weight w11 is 0,6 and w21 is 0,4 then they are associated with an error of -0,79x(0,6/1) and -0,79x(0,4/1) respectively (see Pictures below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/bp_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculation of how much error is associated with every weight we can obtain the errors for the nodes in the proceeding layer.\n",
    "\n",
    "For instance error term for node 1 in the hidden layer will we equal to:\n",
    "\n",
    "the sum of errors associated with all the weights (w11 and w12 in our case) that link this node with the next layer. (see Picture below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/bp_2.png\"  alt=\"Drawing\" style=\"width: 900px;\"/> [Source: https://ebook4expert.com/2016/07/12/make-your-own-neural-network-ebook-free-by-tariq-rashid-epubmobi/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we repeat this procedure for all the nodes in all layers we can find out how much every node should be changed.\n",
    "\n",
    "To do so in Python we just need to make multiplication of vector that contain errors by corresponding matrix of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39519721],\n",
       "       [-1.03021742],\n",
       "       [-0.85068716],\n",
       "       [ 1.33854168],\n",
       "       [-0.08195952],\n",
       "       [-0.68480417],\n",
       "       [-0.60794681],\n",
       "       [-0.29001858],\n",
       "       [ 1.5711066 ],\n",
       "       [-0.3642933 ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the errors associated with hidden layer output:\n",
    "h_errors = np.dot(w_h_o.T, o_errors)\n",
    "h_errors[0:10] # errors in the hidden layer - show the first 10 nodes out of 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Updating weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, how do we improve the weights we have assigned randomly at the beginning, so that the overall result improves?\n",
    "To change the output of any node we should change the weights that connect it with the previous layer.\n",
    "\n",
    "Basically what we want to find out is how much the error in every node changes once we change associated weights. That can be achieved by differentiation of the error function and search for its minimum.\n",
    "\n",
    "The error we want to minimize can be defined as the squared differences between the target value and the output value of the NN. Target value is constant. Output value in its turn is obtained after applying sigmoid function to the sum of weight multiplied by inputs. Following chain rule for derivation our problem can be stated as presented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula2.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula3.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After solving the minimization problem we can update the weights we have assigned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula5.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code this can be represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the matrix for weights between hidden and output layers:\n",
    "w_h_o += np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "# Update the matrix for weights between input and hidden layers:\n",
    "w_i_h += np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the previous code does not display the gif file delete the previous cell and uncomment the following code\n",
    "# import IPython\n",
    "# url = 'https://giphy.com/embed/8tvzvXhB3wcmI'\n",
    "# iframe = '<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>'\n",
    "# IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, there is something else, we forgot when updating our weights. If we completely change our weights with every new observation - our model learns to predict only the last input. Instead of updating weights 100 % every time we can change them only partially - this way every new observation will bring some new knowledge while the previous one will still be in memory even though updated to certain extent. The bigger the learning rate the more importance has the last observation, the smaller it is the more important is all the previous knowledge. The smaller the steps - the more accurate will be the prediction. At the same time it might take more time to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/learning_rate.png\" alt=\"Drawing\" style=\"width: 600px;\"/> [Source: \"Business Analytics & Data Science Course by Professor S. Lessmann, Chapter 5:\n",
    "Artificial Neural Networks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the learning rate\n",
    "l_r = 0.3\n",
    "\n",
    "# update the weights for the links between the hidden and output layers\n",
    "w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "# update the weights for the links between the input and hidden layers\n",
    "w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it in a bigger scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all the steps done before in a loop, so that we can perform them not just for one observation\n",
    "but for all observations in our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    observation = i.split(',')\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "    target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "    target[int(observation[0])] = 0.99\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    o_errors = target - o_output\n",
    "    h_errors = np.dot(w_h_o.T, o_errors)\n",
    "    \n",
    "    w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "    w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the model with all 100 observations we can test it with new data it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the mnist test data CSV file\n",
    "raw_data_test = open(\"data/mnist_test.csv\", 'r')\n",
    "data_test = raw_data_test.readlines()\n",
    "raw_data_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11206cc18>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112001080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check a particular observation\n",
    "observation = data_test[0].split(',')\n",
    "# print the label\n",
    "print(observation[0])\n",
    "# image the number\n",
    "image = np.asfarray(observation[1:]).reshape((28,28))\n",
    "mpp.imshow(image, cmap='Blues', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10333577],\n",
       "       [ 0.02102201],\n",
       "       [ 0.00853034],\n",
       "       [ 0.08203042],\n",
       "       [ 0.00635177],\n",
       "       [ 0.02326992],\n",
       "       [ 0.02360248],\n",
       "       [ 0.42970462],\n",
       "       [ 0.08602563],\n",
       "       [ 0.04961633]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this observation as an input and run NN with it\n",
    "input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "h_input = np.dot(w_i_h, input)\n",
    "h_output = sigmoid(h_input)\n",
    "o_input = np.dot(w_h_o, h_output)\n",
    "o_output = sigmoid(o_input)\n",
    "\n",
    "o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prediction of NN for this test observation\n",
    "label = np.argmax(o_output)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the neural network using all test dataset\n",
    "\n",
    "# scorecard of the network\n",
    "scorecard = []\n",
    "\n",
    "# go through all the observations in the test data set\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    correct_label = int(observation[0])\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    label = np.argmax(o_output)\n",
    "\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.4806\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is several times better than naive. Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What if we train it more? What does this mean? Introduce local minimum concept.\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.9592\n"
     ]
    }
   ],
   "source": [
    "# The \"big loop\" with epochs\n",
    "for e in range(epochs):\n",
    "    for i in data:\n",
    "        observation = i.split(',')\n",
    "        input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "        target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "        target[int(observation[0])] = 0.99\n",
    "\n",
    "        h_input = np.dot(w_i_h, input)\n",
    "        h_output = sigmoid(h_input)\n",
    "        o_input = np.dot(w_h_o, h_output)\n",
    "        o_output = sigmoid(o_input)\n",
    "\n",
    "        o_errors = target - o_output\n",
    "        h_errors = np.dot(w_h_o.T, o_errors)\n",
    "        w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "        w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))\n",
    "\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "# test\n",
    "scorecard = []\n",
    "\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    correct_label = int(observation[0])\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    label = np.argmax(o_output)\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# calculate accuracy\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() /\n",
    "scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other l_r?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_r = 0.1\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More hidden nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_n = 150\n",
    "\n",
    "# Determine the weights for a bigger matrices\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is much easier to train neural networks where the number of neurons is larger than required. But, with a smaller number of neurons the neural network has much better generalization abilities. It means it will respond correctly for patterns not used for training. If too many neurons are used, then the network can be overtrained on the training patterns, but it will fail on patterns never used in training. With a smaller number of neurons, the network cannot be trained to very small errors, but it may produce much better approximations for new patterns. The most common mistake made by many researchers is that in order to speed up the training process and to reduce the training errors, they use neural networks with a larger number of neurons than required. Such networks would perform very poorly for new patterns not used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = open(\"data/mnist_train.csv\", 'r')\n",
    "data = raw_data.readlines()\n",
    "raw_data.close()\n",
    "\n",
    "# Settings\n",
    "epochs = 2\n",
    "l_r = 0.1\n",
    "h_n = 90\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
